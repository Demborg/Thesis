% Created 2018-02-06 tis 11:36
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\newcommand{\bibentry}[1]{\cite{#1}}
\author{Axel Demborg \texttt{demborg@kth.se}}
\date{\today}
\title{Related Work}
\hypersetup{
 pdfauthor={Axel Demborg \texttt{demborg@kth.se}},
 pdftitle={Related Work},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 25.3.1 (Org mode 9.1.6)}, 
 pdflang={English}}
\begin{document}

\maketitle

\section*{Related work}
\label{sec:org15d91b4}
Since their introduction Convolutional Neural Networks\bibentry{lecun1998gradient} have driven the state of the art for computer vision systems. 

It was shown in\bibentry{denil2013predicting} that neural networks typically contain a lot of redundant parameters and that up to 95\% of parameters could be predicted from the remaining 5\% without any drop in accuracy. One of the most direct ways of reducing the number of parameters in the models are the ones where the weight matrices in the networks are dealt with directly, either by grouping weights directly during training with hashing functions \bibentry{chen2015compressing} or by clustering the weights after training\bibentry{gong2014compressing}. Both these approaches reported compression rates of about 20 times before any significant drop in accuracy was introduced which seems to confirm the results from\bibentry{denil2013predicting}. Further work in this area \bibentry{han2015learning} explores the effect of pruning away weights close to zero and thus achieves sparse weight matrices which are not only smaller to store but also faster to compute. Combined with some other tricks like \emph{weight sharing} and \emph{Huffman coding} this lets\bibentry{han2015deep} compress the storage required for AlexNet by a factor 35 and reduce the computation required by a factor 3 wthout any loss in accuracy on the ImageNet dataset.

A different approach for compressing neural networks is to train a small \emph{student} model to mimic a larger \emph{teacher} and thus learn its representations in a more compact form. This idea was introduced in\bibentry{bucilua2006model} to compress an ensemble of models into a single smaller one. Further work along these lines trains very flat student architectures from teachers with deep architectures by regressing to the logits outputs of the teacher. This approach yields fully connected shallow networks with performance previously only achievable from deep convolutional networks \bibentry{ba2014deep}. Improvements to the training of the students were proposed in\bibentry{hinton2015distilling} where regressing to the logits output from the teacher was replaced with a novel approach called \emph{distillation} where the loss function for the student instead is defined as a weighted sum between the cross entropy between the increased temperature softmax outputs from the student and teacher outputs and the cross entropy with the correct labels. 
The results from\bibentry{ba2014deep} were somewhat disproved by \bibentry{urban2016deep} where it is shown that depth and convolutions are central when working with image data in which locality is important. Further work with student-teacher models however experimented with training student models that were thinner but deeper. Training of these \emph{FitNets} was enabled by not only making the student mimic the outputs of the teacher network through distillation but also performing a step of pre-training where an intermediate layer in the student network is tasked with mimicking an early layer in the teacher, thus making the student learn the internal representations of the data from the teacher\bibentry{romero2014fitnets}. This process is called \emph{hint-based training} and manages to produce students with 10 times less parameters but performance that either matches or in some cases even outperform their teachers.


\bibliography{bibliography} 
\bibliographystyle{plain}
\end{document}