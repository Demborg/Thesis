% Created 2018-02-09 fre 17:39
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\newcommand{\bibentry}[1]{\cite{#1}}
\author{Axel Demborg \texttt{demborg@kth.se}}
\date{\today}
\title{Related Work: Network Compression}
\hypersetup{
 pdfauthor={Axel Demborg \texttt{demborg@kth.se}},
 pdftitle={Related Work: Network Compression},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 25.3.1 (Org mode 9.1.6)}, 
 pdflang={English}}
\begin{document}

\maketitle

\section*{Related work}
\label{sec:org6a56c52}
Convolutional Neural Networks (CNN) where first introduced in 1998 \bibentry{lecun1998gradient} and since then larger and larger CNNs have slowly become the state of the art method for most areas of computer vision. Notably \emph{AlexNet} \bibentry{krizhevsky2012imagenet} in 2012 proved that that deep CNNs could be used for high resolution image classification by beating the previous state of the art\bibentry{sanchez2011high} on the \emph{ImageNet} classification challenge\bibentry{deng2009imagenet}. To do this \emph{AlexNet} uses 60 million parameters and 650,000 neurons and training of the network was only made feasible by the use of multiple graphical processing units (GPUs)\bibentry{krizhevsky2012imagenet}. 

In the areas of object detection and semantic segmentation it was \emph{Regions with CNN features} (\emph{R-CNN})\bibentry{girshick2014rich} that in 2014 first showed that CNNs could be successfully be applied to these fields by significantly improving over the previous state of the art in object detection \bibentry{ren2013histograms} and after minor modifications matching the performance of the state of the art in semantic segmentation\bibentry{carreira2012semantic} with a system not specifically built for the task. For object detection \emph{R-CNN} works as a hybrid system with \emph{selective search}\bibentry{uijlings2013selective} producing proposals for object regions and a CNN, pre-trained on \emph{ImageNet}\bibentry{deng2009imagenet} and fine-tuned for region classification, generating fixed length features for each region, finally classifying each region by running class specific \emph{support vector machines}\bibentry{boser1992training} on these features. Some issues with \emph{R-CNN} are that it requires a multistage training, training the CNN to give good features and training the SVMs for classification, and that it is slow, for training but most notably at inference where one image is processed in 47s. These problems are addressed with further work resulting in \emph{Fast R-CNN} \bibentry{girshick2015fast} where the CNN isn't run once per proposed region but instead once for the entire network generating a convolutional feature map that is then pooled with a region of interest (RoI) pooling layer to produce a feature vector for each region. These feature vectors are then feed into a fully connected neural network with two sibling output layers that perform both classification and bounding box refinement in parallel. With these improvements \emph{Fast R-CNN} achieves faster inference and higher accuracy than its predecessors and does so with a arguably much more elegant design. Even though \emph{Fast R-CNN} improved speed significantly it was nowhere near real-time performance, further performance improvements were introduced with \emph{Faster R-CNN}\bibentry{ren2015faster} where \emph{selective search} for region proposals is replaced with Region Proposal Networks, fully convolutional neural networks that take as input the convolutional feature maps as described from \emph{Fast R-CNN} and outputs region proposals. Since this approach for region proposals shares most of its computation with the classification network the region proposals are practically free and frame rates of 5fps are achievable. The region proposal networks not only speed up computation but also prove to give better accuracy region proposals and thus raise over all accuracy in the system as well\bibentry{ren2015faster}. Even further improvements to this framework was achieved with the introduction of \emph{Mask R-CNN}\bibentry{he2017mask} which expands upon \emph{Faster R-CNN} by adding a third branch for a segmentation mask besides the branches for bounding box refinement and classification making the system able to predict not only the general bounding box of items in the image but also which exact pixels belong to the object. Since segmentation is a pixel-by-pixel prediction problem \emph{Mask R-CNN} replaces the spatially quantizing RoIPool operation from \emph{Fast R-CNN} with a quantization-free layer called RoIAllign.

Some parallel work on semantic segmentation of images resulted in \emph{SegNet}\bibentry{badrinarayanan2015segnet}, a fully convolutional encoder-decoder network. Here the encoder network encodes the input image down into a lower dimensional feature space while keeping storing the indices of the max pooling operations. The low dimensional representations are then run through a decoder network which is architecturally a mirror image of the encoder network but where the max pooling operations have been replaced with upsampling layers that use the stored indices from the corresponding pooling layers to maintain the granularity of the images. The final layer in the network is a softmax and hence the outputs are the probabilities of each pixel belong to each class.
Continued work on segmentation utilizes blocks of so called \emph{DenseNets}\bibentry{huang2017densely}, CNNs where every layer is connected to every layer after it enabling the training of really deep network architectures by alleviating the vanishing gradient problem and promoting feature reuse between the layers. By using these \emph{DensNets} in a very deep encoder-decoder structure where skip connections restore image granularity during upsampling the state of the art in image segmentation has been pushed even further \bibentry{jegou2017one} while still reducing the amount of parameters required for the models by a factor 10 as compared to the previous state of art.

Despite their impressive performance on a wide range of problems neural networks are still prohibited from running locally on mobile devices with slow processors, limited power envelopes or limited memory due to their large size and big computational load. For example modern neural networks can't fit on the on-chip SRAM cache and instead they have to reside in the much more power hungry off-chip DRAM memory making applications up to 100 times more power consuming\bibentry{han2015learning}. Regarding inference speed the most modern networks for object segmentation\bibentry{he2017mask} run at 5fps but that is on high performance GPUs meaning that mobile performance is far from real-time. Due to these limitations applications of neural networks for mobile use cases are either to forced give up on state of the art performance or to be run on off-site servers which requires steady network connections and incurs delays, both of which may be intolerable for real-time mobile applications, self driving cars and robotics\bibentry{jin2014flattened}. However work on understanding the structure of the learned weights in neural networks\bibentry{denil2013predicting} has showed that there is significant redundancy in the parameterization of several deep learning models and that up to 95\% of weights in networks can be predicted from the remaining 5\% without any drop in accuracy. This indicates that models could be made much smaller while still maintaining and several such approaches for squeezing high performance networks into small memory footprints and computational loads have been proposed. The most prominent approaches will be presented below. 

\subsection*{Quantization of weights}
\label{sec:org8fe390b}


\subsection*{Weight sharing}
\label{sec:org90d895a}
One of the most direct approaches for removing the redundancy in parametrization from neural networks is by forcing the networks to share weights between different connections. This is precisely what \emph{HashedNets}\bibentry{chen2015compressing} does by fixing the amount of weights \(K^l\) that are to be used in each layer \(\vec{w^l} \in \mathbb{R}^{K^l}\)  and using hashing functions to map each element in the virtual weight matrices \(V_{ij}^l\) to one of these weights \(V_{ij} = w_{h(i,j)}\) with \(h()\) being a hashing function. With the weight matrices defined in this fashion \emph{HashedNets} can be trained like normal networks with the gradients with respect to the weights calculated from the gradients with respect to the virtual matrices as 
\[ \frac{\partial\mathcal{L}}{\partial w_k^l} = \sum_{ij} \frac{\partial\mathcal{L}}{\partial V_{ij}^l}\frac{\partial V_{ij}^l}{\partial w_k^l} \]
This method gave a compression of about 20 times before any notable loss in accuracy was introduced during tests on variations of the MNIST dataset.

Other notable work focuses on the use of k-means clustering to cluster the weights in networks after training\bibentry{gong2014compressing}, this proves to work very well and manages to compress the models with a factor 16 with no more than a 0.5\% drop in classification accuracy on the ImageNet dataset.

\subsection*{Student-teacher learning}
\label{sec:org142d5dc}
Student-teacher learning is a type of model compression where a smaller and/or faster to compute \emph{student} network is trained by making it learn the representations learned by a larger \emph{teacher} network. This idea was first introduced for compressing ensemble models produced by \emph{Ensemble Selection}\bibentry{caruana2004ensemble} which consist of hundreds of models of many different kinds, support vector machines, neural networks, memory based models, and decision trees into a single neural network\bibentry{bucilua2006model}. This work leverages the neural networks property of being universal approximators\bibentry{cybenko1989approximation}, meaning that given sufficiently much training data and a big enough hidden layer a neural network can learn to approximate any function with arbitrary precision, by not directly training the student network om the relatively limited labeled training data available but instead on large amounts of pseudo random data that has been given labels by first being passed through the large teacher ensemble. This compression technique yielded student networks up to 1000 times smaller and 1000 times faster to compute than  their teachers with a negligible drop in accuracy on some test problems.

Further work on student-teacher learning experiments with why deep neural networks usually perform better than shallow ones, even when they have the same amount of parameters, by training shallow student models to mimic deep teachers\bibentry{ba2014deep}. This work introduces two major modifications that make training of these student models feasible, firstly the student model isn't tasked with just recreating the same label as the teacher but also the same distribution which is achieved by regressing the student to the logits, log probability, values of the teacher as they were before softmax. Getting predictions from the student is then achieved by adding a softmax layer to the end of it after training. Secondly a bottleneck linear layer is added to the network to speed up training. With these modifications they are able to train flat neural networks for both the TMIT and CIFAR-10 datasets with performance closely matching that of single deep networks. Continued analysis of flat networks however shows that depth and convolutions are critical for getting good performance on image classification datasets\bibentry{urban2016deep}. Empirically this claim is supported by training state of the art, deep, convolutional models for classification on the CIFAR-10 dataset and then building an ensemble of such models using that as a teacher for shallow students. The student models were then compared to deep convolutional benchmarks that were not trained in a student-teacher fashion. To make sure that the networks were all performing to the best of their abilities and thus making the comparison fair Bayesian hyperparameter optimization\bibentry{snoek2012practical} was used. Through this thorough analysis it was found that shallow networks are unable to mimic the performance of deep networks if the number of parameters is held constant between them, these findings are also in agreement with the theoretical results that the representational efficiency of neural networks grows exponentially with layers\bibentry{liang2016deep}.

Improvements to the student-teacher learning method have been proposed where the student is tasked with minimizing the weighted average of the cross-entropy between its own output and the teacher output when the last layer is softmax with increased temperature, yielding softer labels, and the cross-entropy between the student output and the correct labels when they are available. This framework is called \emph{Distillation}\bibentry{hinton2015distilling} and proves to work very well for transferring of information from teacher to student which is demonstrated by training a student model with only 13.2\% test error on the MNIST dataset despite only having seen 7s and 8s during its own training. These results mean that distillation manages to transfer knowledge about how a 6 looks from the teacher to the student by only telling it to what degree different 7s and 8s don't look like 6s.

Continued work lead to the creation of \emph{FitNets}\bibentry{romero2014fitnets} which goes in the opposite direction to previous attempts at student architectures and instead proposes very deep but thin students. To enable learning in these deep student networks a stage-wise training procedure is used. In the first stage intermediate layers in the teacher and student networks are selected, these are called \emph{hint} and \emph{guided} layers respectively. The guided layer in the student is then tasked to mimic the hint layer in the teacher through a convolutional aggressor that compensates for the difference in number of outputs between the networks, this procedure gives a good initialization for the first layers in the student and allows for it to learn the internal representations of the data from the teacher. The second stage of training is then distillation as described above but with the small addition that the weight of the loss against the teacher is slowly annealed during training. This annealing allows for the student to lean heavily on the teacher for support in early stages of training and learn samples which the even the teacher struggles with towards the end of its training. Using this approach the \emph{FitNets} manage to produce predictions at the same level or in some cases even better than models with 10 times more parameters.

Some of the most resent work in student-teacher learning leverages \emph{attention} mechanisms for the transfer of knowledge allowing the student not only to learn what the teacher sees but also where in the image the teacher looks to see that\bibentry{zagouruyko2017paying}\footnote{måste läsa och faktiskt kunna skiten för att sammanfatta va}. 

\subsection*{Architectural optimizations}
\label{sec:orgc3b24b0}
\bibliographystyle{plain}
\bibliography{bibliography} 
\end{document}