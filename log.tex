% Created 2018-02-05 mån 10:00
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\author{Axel Demborg}
\date{\today}
\title{Thesis log Efficient object segmentation on mobile phones}
\hypersetup{
 pdfauthor={Axel Demborg},
 pdftitle={Thesis log Efficient object segmentation on mobile phones},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 25.3.1 (Org mode 9.1.6)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents




\section{Thesis proposal}
\label{sec:org927801d}
\href{proposal.org}{Proposal}

\section{Concepts}
\label{sec:orgf973fb7}
\begin{description}
\item[{Semantic segmentation}] Take an image and return the class of every pixel in it, don't differentiate between say different cars. Fixed number of classes!
\item[{Object detection}] Take and image and find all the objects and the classes of those, don't do pixel level segmentation but find bounding boxes of the objects. Number of entities is not fixed we have to do stuff like region proposals and classify each region etc.
\item[{Object segmentation}] We want to label each pixel with class and what object it belongs to this is really quite hard..
\item[{Dialated convolution}] The key application the dilated convolution authors have in mind is dense prediction: vision applications where the predicted object that has similar size and structure to the input image. For example, semantic segmentation with one label per pixel; image super-resolution, denoising, demosaicing, bottom-up saliency, keypoint detection, etc. \url{https://arxiv.org/pdf/1511.07122.pdf} related to kronecker product \url{https://arxiv.org/pdf/1512.09194.pdf}.
\end{description}

\section{Literature}
\label{sec:org2b672d0}

\subsection{ImageNet Classification with Deep Convolutional Neural Networks}
\label{sec:orgcf0c940}
\url{http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
hell
\begin{itemize}
\item 5 conv layers 3 fully connected, only about 1\% of weights in each of the conv layers but removing one of them lowers performance significantly. Interesting that so "little" of the network actually is conv.
\item Largest possible model that could fit on two gtx 580, over 60 million params, 1.2 million training examples, 50000 validation and 150000 test. Subset of ImageNet with about 1000 samples from each of 1000 categories.
\item ImageNet is of variable dim but all images were down-sampled to 256 by 256px.
\item Unique approaches in design:
\begin{itemize}
\item ReLU activations. Was one of the first to do this, speeds up training significantly and makes it possible to experiment a bit with models of this size, something that should take far to long using "normal" activations like sigmoid or tanh.
\item Multiple GPUs to hold a model larger than what could fit on a single 3Gb GPU. GPUs only share memory in some selected layers to reduce the time taken for reading memory between them, compare to \emph{columnar CNN}
\item Local Response normalization is a technique for local normalization that is said to help with generalization
\item Let the pooling layers overlap, makes the network slightly harder to overfit
\end{itemize}
\item Combating overfiting, the dataset has 1.2 million images and we have 60 million params in the model making overfiting quite likely. Hence some thought had to be put into reducing overfit.
\begin{itemize}
\item Data augmentation was done to artificially enlarge the training set. Transformed images are calculated on the CPU while the previous batch is run on the GPU and thus this is practically free computationally. Two different label preserving approaches were used. 
\begin{itemize}
\item translations and horizontal reflections where random 224 by 224 patches were extracted from the 256 by 256 images, along with their reflections. The resulting samples are highly correlated but this is really easy to do and reduces overfiting significantly
\item Altering intensities of RGB values, this tries to capture the effect that a object is the same object independent of the intensity or color of the illumination.
\end{itemize}
\item Dropout was performed on the first two fully connected layers with prob 50\% and this reduced overfiting a lot. This however doubles the iterations to convergence (approximately).
\end{itemize}
\item Training was performed using SGD with a minibatch size of 128. The momentum was set to 0.9 and a weight decay was introduced and set to 0.0005. All weights were initialized from a zero mean, 0.01 std Gaussian and some layers got weights initialized to 1 which helps the ReLU:s and the rest were initialized to 0. Learning rate was initialized to 0.01 and reduced three times during training.
\end{itemize}

\subsection{Very Deep Convolutional Networks for Large-Scale Image Recognition.}
\label{sec:org876282a}
\url{https://arxiv.org/pdf/1409.1556/}
\begin{itemize}
\item Small filters (3x3) and lots of layers are better than big filters
\item 224x224 images with RGB
\item Fixed to 3x3 filters (smallest that can capture direction) with stride 1. Some experiments with 1x1 filters (a linear transformation of the image with a added non linearity.), padding done as to preserve dimensions after transformation.
\item 5 max pooling layers with 2x2 windows and stride 2
\item 3 fully connected layers at the end 4096, 4096 and 1000 neurons.
\item No normalization was used in the tested architectures and when tested it didn't seem to improve performance but increases memory usage and computational complexity.
\item ReLU on all hidden layers.
\item Three layers of 3x3 convolution has an effective receptive field of 7x7. The benefit of having multiple layers though is that we have 3 non linearities making for a more discriminative decision function and we also have fewer parameters.
\item Batch size 256 and momentum 0.9.
\item Weight decay 0.0005 was used for the entire network and dropout was applied with 50\% to the first two fully connected layers to perform regularization.
\item Learning rate started at 0.01 and was divided by 10 three times during training.
\item Deep networks can't be trained with poor initialization. To get around this a shallow network was trained using random initialization and the first and last layers from this shallow network were used as initialization and the middle layers randomized. \emph{cool}
\item At test time the FC layers are converted to conv layers (7x7, 1x1, 1x1) and the resulting fully convoluted network can be run on any size image and gives a class score map (Sermanet et al. 2014). The score map is then sum-pooled into a single class.
\item Four titan blacks = 2-3 weeks of computation\ldots{} crap
\end{itemize}

\subsection{Rich feature hierarchies for accurate object detection and semantic segmentation}
\label{sec:org65ec8a0}
\url{https://www.cv-foundation.org/openaccess/content\_cvpr\_2014/papers/Girshick\_Rich\_Feature\_Hierarchies\_2014\_CVPR\_paper.pdf}
\begin{itemize}
\item Generate proposal regions, reshape regions to same format, feed reshaped regions into CNN and get features, class specific SVM classifiers then return the class.
\item Insight: \emph{Transfer learning} when data is scarce we can train supervised on an other dataset in this case ILSVRC and then do domain specific fine tuning with the data ẃe have.
\item \emph{We “lobotomized” the CNN and found that a surprisingly large proportion, 94\%, of its parameters can be removed with only a moderate drop in detection accuracy.} Hmmmm, very interesting
\item Region proposal through selective search, fast mode (gives \textasciitilde{}2000 props)
\item Feature extraction by running each region through a CNN with five conv layers and three fc layers. Regions are warped to fit in a 227x227 input to the CNN
\item Given all the scored regions we do  a greedy non-maximum suppression (for each class independently) that rejects a region if it has an intersection-over-union (IoU) overlap with a higher scoring selected region larger than a learned threshold.
\item This is fast since all the heavy operations (the CNN) are shared between all classes and the only class specific operations are the ones associated with the SVM.
\item For fine tuning the 1000 way classification layer is replaced with a 21 way one, 20 classes + bg. The minibatches are created by randomly sampling 32 positive windows and 96 bg windows. This bias towards positive windows is important since they are so rare. Training rate is started at 0.001 (1/10 of pre-training rate). Samples with IoU (Intersect over Union) of 50\% or more are considered positive.
\item Nice tool for error visualization
\item Some experiments with semantic segmentation, this is done with three approaches
\begin{description}
\item[{full}] Calculate for the rectangular bounding box of the mask
\item[{fg}] Calculate only for the masked pixels and replace the other pixels with the image mean so that they become zero after mean subtraction.
\item[{full + fg}] concatenates the features form the two. This works the best.
\end{description}
\item Results were achieved by letting classical tools from CV and CNN:s instead of treating them as opposing lines of scientific inquiry.
\end{itemize}


\subsubsection{Fast R-CNN}
\label{sec:org1f08276}
\url{https://www.cv-foundation.org/openaccess/content\_iccv\_2015/papers/Girshick\_Fast\_R-CNN\_ICCV\_2015\_paper.pdf}
\url{https://github.com/rbgirshick/fast-rcnn}

\begin{itemize}
\item R-CNN is cool but has some drawbacks:
\begin{itemize}
\item Training is multistage
\item Training is expensive in space and time
\item Object detection is \emph{slow} 47s/image on GPU.
\end{itemize}
\item R-CNN is slow since the CNN forward pass has to be performed once for each proposal in the image (\textasciitilde{}2000) and computations are not shared between these. SPPnet is an attempt to mitigate this but has some drawbacks of its own like like fixed convolutional layers.
\item Proposes new training algorithm \emph{Fast R-CNN} that solves issues above.
\begin{itemize}
\item Input is the entire image along with region proposals
\item The entire image is run through a bunch of conv layers to produce a feature map.
\item For each object proposal region of interest (RoI) pooling is performed to produce a fixed length feature vector.
\item Feature vectors are then feed into a sequence of fc layers that branch into two output output layers. One for softmax class probs and one with four values that code for the bounding box of the region.
\item Smooth L\(_{\text{1}}\) loss is less sensitive than L\(_{\text{2}}\) loss when using unbounded targets
\item A momentum of 0.9 and parameter decay of 0.0005 (on weights and biases) are used.
\end{itemize}
\end{itemize}


\subsubsection{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks \url{http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf}}
\label{sec:org41bafea}
\url{https://github.com/rbgirshick/py-faster-rcnn}
\begin{itemize}
\item Previous improvements in object detection have focused on what we do after we have some object proposals from older methods, this has lead to object proposals now being the bottleneck. In this article a approach is presented where a fully convolutional models replaces the previous regions proposals with a region proposal network (RPN) that has the improvement that it can share features with the object classifier, thus enabling almost cost free region proposals.
\item We can use the feature map from Fast R-CNN and do region proposals from those.
\item Training alternates between fine tuning for region proposals and object detection.
\item Achieves 5 fps.
\item RPN takes an image of any size and outputs a set of object rectangular proposals with associated objectness score.
\item To generate region props a small fc network is slided over the conv feature map of the last shared layer. Typically this looks at 3x3 pixels (since the network above is large this corresponds to a large receptive field) and outputs a lower dimensional vector (256-d or 512-d) this signature vector is then feed into two sibling fc networks that outputs the candidate box and the objectness score. This is implemented as a CNN with 3x3 receptive field and 256 output dims followed by two different 1x1 CNN layers.
\item At each sliding window location we predict k regions props so the reg layer has 4k outputs encoding the corners of the k regions and the cls network has 2k outputs coding the probability of each region being bg or object
\item The k proposals are parametrized relative to k reference boxes called anchors. Each anchor is centered at the sliding window in question and is associated with a scale and a aspect ratio (Whaat??). The article uses 3 scales and 3 aspect ratios.
\item Trained using a pragmatic four step approach
\begin{enumerate}
\item Simply train a RPN for its own
\item Train a separate network for Fast R-CNN using props generated from 1)
\item Now use 2) to intitialize training for a new RPN but fix the conv layers and only train layers unique to RPN.
\item Finally use 3) to fintune the layers unique to Fast R-CNN
\end{enumerate}
\end{itemize}
\subsubsection{Mask R-CNN}
\label{sec:orgb42fcd8}
\url{https://arxiv.org/pdf/1703.06870.pdf}

\begin{itemize}
\item Extends Faster R-CNN by adding a branch for predicting an object mask.
\item Mask is created by a FCN model
\item RoIPool from R-CNN is replaced with RoIAllign which doesn't loose much spatial information and allows for pixel alignment on the masks.
\item[{Human pose estimation}] One hot binary mask is used for each key point. Minimize cross-entropy loss over m\(^{\text{2}}\)-way softmax which encourages a single point being predicted.
\end{itemize}

\subsection{Learning to Segment Object Candidates}
\label{sec:orga8c1807}
\url{http://papers.nips.cc/paper/5852-learning-to-segment-object-candidates.pdf}
\begin{itemize}
\item Not so fun? feels a bit off?
\end{itemize}

\subsubsection{Learning to Refine Object Segments}
\label{sec:orgaf3ebc9}
\url{https://arxiv.org/pdf/1603.08695.pdf}
\begin{itemize}
\item Output a course mask in a feed forward pass then refine it using features from successfully deeper layers.
\end{itemize}

\subsection{SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation}
\label{sec:org16d36bf}
\url{https://arxiv.org/pdf/1511.00561.pdf}

\begin{itemize}
\item Fully convolutional, encoder-decoder. Encoder topologically the 13 conv layers from the VGG16 network. This is then decoded through usage of the pooling indices and some convolutions to produce a new feature map of input size in which we can do pixel-wise classification.
\item Designed to be efficient in memory and computation during inference.
\item Very detailed literature review 2016
\item Uses batch norm
\item Storing the indices from pooling uses way less memory than storing the entire feature maps would, 2 bits vs 4 floats per max pool.
\item Stuff like meadian frequency balancing for classes that are not as common?
\end{itemize}

\subsection{ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation}
\label{sec:org5f477db}
\url{https://arxiv.org/pdf/1606.02147.pdf}

\begin{itemize}
\item Optimized for mobile performance!
\item Batch norm and PReLU (leaky ReLU)
\item No biases used in the projections
\item Strong downsampling makes for large receptive fields which is useful for context, however it makes for a loss in resolution that hurts when we want to do pixel-level segmentation in the input image. \emph{dilated convolutions} might help?
\item It's heavy to work with large images, hence we want to do downsampling early and reduce the amount of data we have to process. Early downsampling layers shouldn't contribute to classification but instead act as powerful feature extractors.
\item In SegNet encoder/decoder are symmetrical, this might not be necessary here a way smaller decoder is used since its task should be simpler, only upsample.
\item Convolutional filters can be \emph{factorized} a nxn filter can be replaced with a 1xn followed by a nx1 which is lighter to compute and in many cases just as good.
\item Dilated convolutions?! \href{http://www.inference.vc/dilated-convolutions-and-kronecker-factorisation/}{some blogg}
\item Regularization using Spatial Dropout
\item Heyo, finally someone uses ADAM
\end{itemize}

\subsection{The One Hundred Layers Tiramisu: FC DenseNets for Semantic Segmentation}
\label{sec:org3e9162b}
\url{https://arxiv.org/pdf/1611.09326.pdf}
\url{https://github.com/SimJeg/FC-DenseNet}

\begin{itemize}
\item \href{https://arxiv.org/pdf/1608.06993.pdf}{DensNets}, every layer is directly connected to every other in a feed-forward fashion.
\item State of the art CNN:s heavily reduce dimensionality by pooling layers which is good if we want to predict a single label. Not good for dense prediction
\item FCN:s or Fully Convolutional Networks work with downsampling and upsammpling, adding skip connections between layers to keep fine-grained information.
\item DensNet architecture is extended to be fully convolutional, mitigating feature map explosion.
\item Models were trained from scratch!
\item weight decay 1e-4 and dropout 0.2
\item Feature maps are concatenated after all layers, not in upsampling though.
\item Very deep networks \textasciitilde{}103 layers and quite few parameters, 10 fold reduction compared to state of the art.
\end{itemize}

\subsection{Exploiting Local Structures with the Kronecker Layer in Convolutional Networks}
\label{sec:org304983c}
\url{https://arxiv.org/pdf/1512.09194.pdf}

\begin{itemize}
\item Approximating weight mx with kronecker product of two other mx:es
\end{itemize}

\subsection{Dialated Convolutions}
\label{sec:org55a1cb5}
\url{https://arxiv.org/pdf/1511.07122.pdf}

\begin{itemize}
\item Traditional convolutions are designed for problems like image classification, problems that require \emph{dense estimation} however need good spatial accuracy and a wide receptive fields. Dialated convolution tries to fill this gap.
\item The dilated conv operator can apply the same filter at multiple scales using different dilation factors!
\item Random initialization works poorly, instead do some sort of identity initialization.
\item We still have to train the models for image classification in the start meaning networks with such proprieties will stay popular.
\end{itemize}

\subsection{Maxout networks\hfill{}\textsc{Interesting}}
\label{sec:orgfcfad91}
\url{https://arxiv.org/pdf/1302.4389.pdf}

\begin{itemize}
\item Architecture directly dessigned for dropout
\end{itemize}
\subsection{Predicting parameters in deep learning}
\label{sec:org32da8aa}
\url{http://papers.nips.cc/paper/5025-predicting-parameters-in-deep-learning.pdf}

\begin{itemize}
\item Weights in neural networks tend to be structured, this can be used.
\item It is highly likely that a pixel in an image is the weighted average of it's neighbors. If we use this fact we don't have to store weights for every input.
\item One way of representing a weight matrix in a sparse way is in a factorized format \(W = UV\) however there is redundancy in that \(W = UV = (UQ)(Q^{-1}V)\) so one way of making the hproblem well defined is by selecting U and only learn V, but how?
\item U becomes a dictionary of basis functions, what is a good choice? We can use previous knowledge to build U, either as a selection of Fourier wavelets that encode a notion of smoothness or as some kernel function.
\item In the best case we are able to predict more than 95\% of the weights of a network without any drop in accuracy.
\end{itemize}

\subsection{Compressing Deep Neural Convolutional Networks using Vector Quantization}
\label{sec:orgf100e6f}
\url{https://arxiv.org/pdf/1412.6115.pdf}

\begin{itemize}
\item Simply applying k-means clustering on the weights or conducting product quantization can lead to a very good balance between model size and accuracy. 16-24 times reduction in size with a 1\% loss in accuracy.
\item Models typicaly in the range of 200M but almost nobody downloads apps over 20M, compression required for feasability.
\item Models are hevily over parametrized? \href{http://papers.nips.cc/paper/5025-predicting-parameters-in-deep-learning.pdf}{(Denil et al., 2013)} layers within one layer can be predicted from a subset of 5\% of the weights.
\item In general we have 90\% of weights in FC layers and 90\% of running time in CNN layers (Zeiler \& Fergus, 2013). This means that we speed up networks up working on the convolutional layers and make them smaler by working on the fully connected layers.
\item Scalar quantization from k-means and structred quantization from prroduct quantization or residual quantization.
\item Two paths for compressing parameters:
\begin{description}
\item[{Matrix factorisation}] SVD on parameter matrix. Has sucessfully been applied to speeding up CNN:s
\item[{Vector Quantization}] Has a few variants
\begin{description}
\item[{Binarization}] Relly aggresive technique \[ \hat{W_{ij}} = 1 ~ \textit{if} ~ W_{ij} \geq 0 ~ \textit{else} -1 \] This will compress data by a factor 32 since every float32 is represented as a single bit.
\item[{k-means}] Do k-means on the values in each weight mx, store a code book plus the index of each weight. Compression factor is \(32 / \log_2(k)\) assuming the size of the code book is negligible. Surprisingly good results for such a simple model.
\item[{product quantization}] From \href{https://lear.inrialpes.fr/pubs/2011/JDS11/jegou\_searching\_with\_quantization.pdf}{(Jegou et al., 2011)}
\item[{residual quantization}] From \href{http://www.mdpi.com/1424-8220/10/12/11259/htm}{(Chen et al., 2010)}
\end{description}
\end{description}
\end{itemize}

\subsection{Do Deep Nets Really Need to be Deep?}
\label{sec:orgfdfbdf7}
\url{http://papers.nips.cc/paper/5484-do-deep-nets-really-need-to-be-deep.pdf}

\begin{itemize}
\item \emph{Why} do deep convolutional networks work better than shallow fully connected ones? Well they aren't really
\item We can build mimic models that train on the soft output from larger networks instead of the actual targets. This can give smaller networks with the same performance even tough we could not have trained that smaller network from scratch. This means that the function is \emph{learnable} by a small network but that current training algorithms can't find the right settings!
\item We train on the logits values before soft max. Easier to learn for the student model if we skip the information loss that passing logits to prob space introduces.
\item Normalizing targets improves L2 loss slightly.
\item A linear bottleneck layer added between input and hidden layer, this works as a factorisation of the weight mx and both reduces training time and memory consumption.
\item model compression works best when the unlabeled set is very large, and when the unlabeled samples do not fall on train points where the teacher model is likely to have overfit.
\item Why does this work?! Well, erroneous lables may be filtered away by the teacher and the soft lables contain more information than the one-hot alternatives. These mechanisms work like regularization, preventing the student from overfiting.
\item Important to note that unlabeled data can be used for transfer learning from teacher to student.
\item The shallow models train faster and more importantly can execute way better in parallel at inference time than the deep models when we can't batch process.
\end{itemize}
\subsection{Distilling the Knowledge in a Neural Network}
\label{sec:org72d8131}
\url{https://arxiv.org/pdf/1503.02531.pdf}
\begin{itemize}
\item Train a large cumbersome model to understand that data and then distill it for deployment.
\item It is blocking to view the knowledge in the network as the weights learned. If we instead think of it as a mapping from input to output vectors it is easier to see how this could be distilled.
\item Instead of training on logits the temperature of the softmax is raised until the output is sufficiently smooth.
\item A combination of soft (output from teacher) and hard (actual) lables can be used with some gain.
\item Experiments were done by only using 7 and 8 in the transfer set. This gives a huge error of 47.3\% on the test set. Most of this is due to incorrect biases though which is demonstrated by reducing the biases of 7 and 8 by 7.6 (optimizing test performance) and getting 13.2\% errors! This is extraordinary! By never having seen most of the digits and just being told which 7 and 8 look like other digits and how mush the student learns quite well how a 3 looks, whaat?!
\end{itemize}

\subsection{Compressing Neural Networks with the Hashing Trick}
\label{sec:org2799262}
\url{http://proceedings.mlr.press/v37/chenc15.pdf}
\begin{itemize}
\item The trend in deep learning is to build bigger and bigger models absorbing ever growing datasets but we want to run stuff on mobile devices with limited memory etc, how do we handle this? HashedNets exploit the inherent redundancy in neural networks and randomly groups connection weights into buckets. All weights in one bucket share one parameter value.
\item Large models don't fit in the memory of mobile phones and we either have to transfer the data and do testing in the cloud (requires good internet, might share sensitive data, takes time) or train smaller models for mobile devices (bad performance, customers angry). We want to \emph{Compress neural networks}
\item Get better performance by training on soft targets from bigger NN
\end{itemize}

\subsection{Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding\hfill{}\textsc{Cool}}
\label{sec:org810dd3a}
\url{https://arxiv.org/pdf/1510.00149.pdf}

\begin{itemize}
\item DNN:s are computationally and memory intense, bad for mobile devices
\item Compression in a three stage process
\begin{description}
\item[{Pruning}] learn only important connections
\item[{Quantize weights}] Enforces weight sharing
\item[{Apply Huffman coding}] Takes advantage of biased distribution of weights
\end{description}
\item Pruning is performed by first training the network in a normal way then pruning away all the small-weight connections. The network is then retrained to learn the final values for the kept weights. weights are then stored as a sparse mx
\item Weights are clustered and and the weights within each cluster are set to be the same (mean of the real weights). a final round of training is then performed to make these centroid weights find their correct values.
\end{itemize}

\subsection{Learning both Weights and Connections for Efficient Neural Networks}
\label{sec:org21d3f5d}
\url{http://papers.nips.cc/paper/5784-learning-both-weights-and-connections-for-efficient-neural-network.pdf}

\begin{itemize}
\item Neural network are heavy and slow, yeah, we know.
\item Prohibative for mobile, especially if we look at energy cost. Energy consumption is mainly form memory access and if we cant fit on the on chip SRAM (5 pJ/access) we have to go to off chip DRAM (640 pJ/access). So if we can fit our models on SRAM we can save a factor 100 on power for some applications.
\item Pruning of unimportant weights and retraining. Some biological similarity in the first months of a childs development.
\item Prune weights below a threshold, retrain after pruning.
\item L1 gives better performance before retraining since some wieghts are already forced to 0, however the quality of kept weights are poorer and after retraining we get better results from L2
\item Dropout rate has to be reduced for retraining proposed equation for this provided.
\item Iterative pruning finally gives optimal results.
\item Pruning threshold is taken as a quality parameter multiplied by the std of the layers weights.
\item retrain with 1/10 learning rate.
\end{itemize}

\subsection{FitNets: Hints for Thin Deep Nets\hfill{}\textsc{Cool}}
\label{sec:orgb8253d6}
\url{https://arxiv.org/abs/1412.6550}

\begin{itemize}
\item Students are thin and deep networks, fewer parameters and faster execution
\item Other approches flatten the networks or keep the same depth but deepth is very expressive, why not go deeper?
\item Supervission at intermidiate layers has proven to be helpful (Chen-Yu et al., 2014; Szegedy et al., 2014; Gulcehre \& Bengio, 2013)
\item Student is thiner and deeper than teacher, also gets intermediate-level hints to learn good representations.
\item Trains like destilation with relaxed softmax, not directly on logits and loss is sum over true and soft labels
\item Can't train deeper students directly
\begin{itemize}
\item Hints solve this problem by making a intermediate layer in the student network model a intermediate layer in the teacher. This is a form of regularization and the deeper the layer we link is the stiffer the student becomes. In the paper the middle layers of student and teacher are linked.
\item The teacher layer might be bigger than the student one so a regressor is added to make the layers comparable.
\item The FitNet is then trained up untill the guided layer by minimizing L2 error between teachers hint layer and students guided layer with added regressor.
\item A conv regressor is used.
\end{itemize}
\item Trained stage-wise, first train hints then train the entire network to minimize destilation loss.
\item Relation to curricilum learning
\begin{itemize}
\item The weight of the teachers error \(\lambda\) is annealed linearly during training allowing for examples on which the teacher is unsure have a litle impact early on and get more and more impactful as it decays. This means that easy samples are weighted heavily in the start making for a solid foundation and then hard samples get their place and the network keeps improving.
\item Uses maxout layers \url{https://arxiv.org/pdf/1302.4389.pdf}
\item Given a computational budget deeper networks are better than shallow ones
\end{itemize}
\end{itemize}
\subsection{DO DEEP CONVOLUTIONAL NETS REALLY NEED TO BE DEEP AND CONVOLUTIONAL?}
\label{sec:org354974a}
\url{https://arxiv.org/pdf/1603.05691.pdf}

\begin{itemize}
\item Yes they do! oh, crap we can't just flatten out everything :(
\item Ba and Cuna (Do deep nets really..) demonstrate good results on TMIT that is not that dependant on convolution however they have a harder time working on CIFAR10
\item Bayesian optimization to explore architecture and hyper param space implementation \href{https://github.com/JasperSnoek/spearmint}{Spearmint}
\item Checkout \emph{FitNets (romero et al 2015)}
\item Teacher ensamble gets 93.8\% on CIFAR10
\item No weight decay or dropout for students, has proven to just hinder performance
\item Huge gap between students with and without convolutions, they appear to be crucial. The gains from soft labels also decreese with student models more similar to the teacher.
\item Although the shallow models are not as accurate as the deep ones they are still the best trained in their respective categories.
\item Dropout gave consitantly worse results when training the students and indicates that soft lables work as a powerful regualizer
\end{itemize}
\subsection{MobileNets: Effincient Convolutional Neural Networks for Mobile Vision Applications\hfill{}\textsc{Cool}}
\label{sec:orgde04d1b}
\url{https://arxiv.org/pdf/1704.04861.pdf}
Some keras code \url{https://github.com/keras-team/keras/blob/master/keras/applications/mobilenet.py}
\begin{itemize}
\item Focus on speed but also yeilds small networks
\item \emph{Depthwise serparable convolutions}
\item Litle regularization and data augmentation used since small models don't suffer that badly from overfitting
\item width multiplier \(\alpha\) is used to define a new uniformly thined network, has to be retrained from scratch
\item resolution multiplier \(\rho\) usually set implicilty by scaling input images
\item Also used with destilation and works great!
\end{itemize}
\section{Meetings}
\label{sec:org9d50474}
\subsection{{\bfseries\sffamily DONE} Meeting 1}
\label{sec:orgb57d439}
\begin{itemize}
\item segmentation, few classes like four
\item lots of variation in data
\item good data set of forgorund, rgb feet images from scanners --> synthetic dataset
\item Testing image segmentation state of the art
\item Realtime segmentation = realtime feedback
\item One possible output is running on the scanners
\item Fine to but images in repport, same for numbers if anything we want to brag about it
\item For specification more detail and do time plan
\item Formalize for specification, what is good?
\item This is a bit open ended what we can expect
\item Ensamble as teacher might expect better than individuals for
\item hintsnet
\item Checkpoints
\begin{itemize}
\item Do specification mention options
\item Do related work
\item Halfway seminar
\end{itemize}
\end{itemize}
\end{document}