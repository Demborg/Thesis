#+TITLE: Related Work
#+AUTHOR: Axel Demborg \texttt{demborg@kth.se}
#+LATEX_HEADER: \newcommand{\bibentry}[1]{\cite{#1}}
#+OPTIONS: toc:nil

Since their introduction Convolutional Neural Networks[[bibentry:lecun1998gradient]] have driven the state of the art for computer vision systems. 
# blal bla grejer om previous work på object segmentation

# Nått om reduced bit representation

It was shown in[[bibentry:denil2013predicting]] that neural networks typically contain a lot of redundant parameters and that up to 95% of parameters could be predicted from the remaining 5% without any drop in accuracy. Since then there have been several attempts to try and remove this redundancy. 

# prata om 2) clustering parameters and pruning. 3) Student-teacher 4) Aproximations of convolution

A different approach for compressing neural networks is to train a small /student/ model to mimic a larger /teacher/ and thus learn its representations in a more compact form. This idea was introduced in[[bibentry:bucilua2006model]] to compress an ensemble of models into a single smaller one. Further work along these lines trains very flat student architectures from teachers with deep architectures by regressing to the logits outputs of the teacher. This approach yields fully connected shallow networks with performance previously only achievable from deep convolutional networks [[bibentry:ba2014deep]]. Improvements to the training of the students were proposed in[[bibentry:hinton2015distilling]] where regressing to the logits output from the teacher was replaced with a novel approach called /distillation/ where the loss function for the student instead is defined as a weighted sum between the cross entropy between the increased temperature softmax outputs from the student and teacher outputs and the cross entropy with the correct labels. 
The results from[[bibentry:ba2014deep]] were somewhat disproved by [[bibentry:urban2016deep]] where it is shown that depth and convolutions are central when working with image data in which locality is important. Further work with student-teacher models however experimented with training student models that were thinner but deeper. Training of these /FitNets/ was enabled by not only making the student mimic the outputs of the teacher network through distillation but also performing a step of pre-training where an intermediate layer in the student network is tasked with mimicking an early layer in the teacher, thus making the student learn the internal representations of the data from the teacher[[bibentry:romero2014fitnets]]. This process is called /hint-based training/ and manages to produce students with 10 times less parameters but performance that either matches or in some cases even outperform their teachers.

[[bibliography:bibliography.bib]] 
[[bibliographystyle:plain]]
