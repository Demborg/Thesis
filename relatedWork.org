#+TITLE: Related Work: Network Compression
#+AUTHOR: Axel Demborg \texttt{demborg@kth.se}
#+LATEX_HEADER: \newcommand{\bibentry}[1]{\cite{#1}}
# #+LATEX_HEADER: \usepackage{biblatex}
#+OPTIONS: toc:nil num:nil

* Related work
# Since their introduction Convolutional Neural Networks (CNNs)[[bibentry:lecun1998gradient]] have driven the state of the art for computer vision systems. Initially CNNs revolutionized image classification[[bibentry:krizhevsky2012imagenet]] but more lately they have successfully been deployed on object segmentation[[bibentry:girshick2015fast]] [[bibentry:ren2015faster]] [[bibentry:badrinarayanan2015segnet]] [[bibentry:he2017mask]] becoming the state of the art method in this field to. Despite their impressive results however these modern neural networks have a few drawbacks that prevent them from getting widespread use, namely size and speed, making them far to demanding to run on mobile devices such as smartphones[[bibentry:han2015learning]].

Convolutional Neural Networks (CNN) where first introduced in 1998 [[bibentry:lecun1998gradient]] and since then larger and larger CNNs have slowly become the state of the art method for most areas of computer vision. Notably /AlexNet/ [[bibentry:krizhevsky2012imagenet]] in 2012 proved that that deep CNNs could be used for high resolution image classification by beating the previous state of the art[[bibentry:sanchez2011high]] on the /ImageNet/ classification challenge[[bibentry:deng2009imagenet]]. To do this /AlexNet/ uses 60 million parameters and 650,000 neurons and training of the network was only made feasible by the use of multiple graphical processing units (GPUs)[[bibentry:krizhevsky2012imagenet]]. 

In the areas of object detection and semantic segmentation it was /Regions with CNN features/ (/R-CNN/)[[bibentry:girshick2014rich]] that in 2014 first showed that CNNs could be successfully be applied to these fields by significantly improving over the previous state of the art in object detection [[bibentry:ren2013histograms]] and after minor modifications matching the performance of the state of the art in semantic segmentation[[bibentry:carreira2012semantic]] with a system not specifically built for the task. For object detection /R-CNN/ works as a hybrid system with /selective search/[[bibentry:uijlings2013selective]] producing proposals for object regions and a CNN, pre-trained on /ImageNet/[[bibentry:deng2009imagenet]] and fine-tuned for region classification, generating fixed length features for each region, finally classifying each region by running class specific /support vector machines/[[bibentry:boser1992training]] on these features. Some issues with /R-CNN/ are that it requires a multistage training, training the CNN to give good features and training the SVMs for classification, and that it is slow, for training but most notably at inference where one image is processed in 47s. These problems are addressed with further work resulting in /Fast R-CNN/ [[bibentry:girshick2015fast]] where the CNN isn't run once per proposed region but instead once for the entire network generating a convolutional feature map that is then pooled with a region of interest (RoI) pooling layer to produce a feature vector for each region. These feature vectors are then feed into a fully connected neural network with two sibling output layers that perform both classification and bounding box refinement in parallel. With these improvements /Fast R-CNN/ achieves faster inference and higher accuracy than its predecessors and does so with a arguably much more elegant design. Even though /Fast R-CNN/ improved speed significantly it was nowhere near real-time performance, further performance improvements were introduced with /Faster R-CNN/[[bibentry:ren2015faster]] where /selective search/ for region proposals is replaced with Region Proposal Networks, fully convolutional neural networks that take as input the convolutional feature maps as described from /Fast R-CNN/ and outputs region proposals. Since this approach for region proposals shares most of its computation with the classification network the region proposals are practically free and frame rates of 5fps are achievable. The region proposal networks not only speed up computation but also prove to give better accuracy region proposals and thus raise over all accuracy in the system as well[[bibentry:ren2015faster]]. Even further improvements to this framework was achieved with the introduction of /Mask R-CNN/[[bibentry:he2017mask]] which expands upon /Faster R-CNN/ by adding a third branch for a segmentation mask besides the branches for bounding box refinement and classification making the system able to predict not only the general bounding box of items in the image but also which exact pixels belong to the object. Since segmentation is a pixel-by-pixel prediction problem /Mask R-CNN/ replaces the spatially quantizing RoIPool operation from /Fast R-CNN/ with a quantization-free layer called RoIAllign.

Some parallel work on semantic segmentation of images resulted in /SegNet/[[bibentry:badrinarayanan2015segnet]], a fully convolutional encoder-decoder network. Here the encoder network encodes the input image down into a lower dimensional feature space while keeping storing the indices of the max pooling operations. The low dimensional representations are then run through a decoder network which is architecturally a mirror image of the encoder network but where the max pooling operations have been replaced with upsampling layers that use the stored indices from the corresponding pooling layers to maintain the granularity of the images. The final layer in the network is a softmax and hence the outputs are the probabilities of each pixel belong to each class.
Continued work on segmentation utilizes blocks of so called /DenseNets/[[bibentry:huang2017densely]], CNNs where every layer is connected to every layer after it enabling the training of really deep network architectures by alleviating the vanishing gradient problem and promoting feature reuse between the layers. By using these /DensNets/ in a very deep encoder-decoder structure where skip connections restore image granularity during upsampling the state of the art in image segmentation has been pushed even further [[bibentry:jegou2017one]] while still reducing the amount of parameters required for the models by a factor 10 as compared to the previous state of art.

Despite their impressive performance on a wide range of problems neural networks are still prohibited from running locally on mobile devices with slow processors, limited power envelopes or limited memory due to their large size and big computational load. For example modern neural networks can't fit on the on-chip SRAM cache and instead they have to reside in the much more power hungry off-chip DRAM memory making applications up to 100 times more power consuming[[bibentry:han2015learning]]. Regarding inference speed the most modern networks for object segmentation[[bibentry:he2017mask]] run at 5fps but that is on high performance GPUs meaning that mobile performance is far from real-time. Due to these limitations applications of neural networks for mobile use cases are either to forced give up on state of the art performance or to be run on off-site servers which requires steady network connections and incurs delays, both of which may be intolerable for real-time mobile applications, self driving cars and robotics[[bibentry:jin2014flattened]]. However work on understanding the structure of the learned weights in neural networks[[bibentry:denil2013predicting]] has showed that there is significant redundancy in the parameterization of several deep learning models and that up to 95% of weights in networks can be predicted from the remaining 5% without any drop in accuracy. This indicates that models could be made much smaller while still maintaining and several such approaches for squeezing high performance networks into small memory footprints and computational loads have been proposed. The most prominent approaches will be presented below. 

** Quantization of weights
# Modern neural networks are usually based on 32-bit floating point representations of parameters. It has been shown however that networks are quite resilient to noise and even that some noise can improve training[[bibentry:murray1994enhanced]]. Since reduced precision variables can be modeled as noise this means that networks can be compressed by changing to a less accurate format without any loss in performance. This can be done either by reducing the bit accuracy after training[[bibentry:vanhoucke2011improving]]  or by doing the entire training in reduced accuracy[[bibentry:hubara2016quantized]] [[bibentry:gupta2015deep]]. The benefits of using a reduced format like this for representation is not only that the models take less space but also that the individual multiplications become cheaper and hence the networks run faster.


** Weight sharing
# It was shown in[[bibentry:denil2013predicting]] that neural networks typically contain a lot of redundant parameters and that up to 95% of parameters could be predicted from the remaining 5% without any drop in accuracy. One of the most direct ways of reducing the number of parameters in the models ones where the weight matrices in the networks are dealt with directly, either by grouping weights during training with hashing functions [[bibentry:chen2015compressing]] or by clustering the weights after training[[bibentry:gong2014compressing]]. Both these approaches reported compression rates of about 20 times before any significant drop in accuracy was introduced which seems to confirm the results from[[bibentry:denil2013predicting]]. Further work in this area [[bibentry:han2015learning]] explores the effect of pruning away weights close to zero and thus achieves sparse weight matrices which are not only smaller to store but also faster to compute. Combined with some other tricks like /weight sharing/ and /Huffman coding/ this lets[[bibentry:han2015deep]] compress the storage required for AlexNet by a factor 35 and reduce the computation required by a factor 3 without any loss in accuracy on the ImageNet dataset.

One of the most direct approaches for removing the redundancy in parametrization from neural networks is by forcing the networks to share weights between different connections. This is precisely what /HashedNets/[[bibentry:chen2015compressing]] does by fixing the amount of weights \(K^l\) that are to be used in each layer \(\vec{w^l} \in \mathbb{R}^{K^l}\)  and using hashing functions to map each element in the virtual weight matrices \(V_{ij}^l\) to one of these weights \(V_{ij} = w_{h(i,j)} \) with \(h()\) being a hashing function. With the weight matrices defined in this fashion /HashedNets/ can be trained like normal networks with the gradients with respect to the weights calculated from the gradients with respect to the virtual matrices as 
\[ \frac{\partial\mathcal{L}}{\partial w_k^l} = \sum_{ij} \frac{\partial\mathcal{L}}{\partial V_{ij}^l}\frac{\partial V_{ij}^l}{\partial w_k^l} \]
This method gave a compression of about 20 times before any notable loss in accuracy was introduced during tests on variations of the MNIST dataset.

Other notable work focuses on the use of k-means clustering to cluster the weights in networks after training[[bibentry:gong2014compressing]], this proves to work very well and manages to compress the models with a factor 16 with no more than a 0.5% drop in classification accuracy on the ImageNet dataset.

** Student-teacher learning
# A different approach for compressing neural networks is to train a small /student/ model to mimic a larger /teacher/ and thus learn its representations in a more compact form. This idea was introduced in[[bibentry:bucilua2006model]] to compress an ensemble of models into a single smaller one. Further work along these lines trains very flat student architectures from teachers with deep architectures by regressing to the logits outputs of the teacher. This approach yields fully connected shallow networks with performance previously only achievable from deep convolutional networks [[bibentry:ba2014deep]]. Improvements to the training of the students were proposed in[[bibentry:hinton2015distilling]] where regressing to the logits output from the teacher was replaced with a novel approach called /distillation/ where the loss function for the student instead is defined as a weighted sum between the cross entropy between the increased temperature softmax outputs from the student and teacher outputs and the cross entropy with the correct labels. 
# The results from[[bibentry:ba2014deep]] were somewhat disproved by [[bibentry:urban2016deep]] where it is shown that depth and convolutions are central when working with image data in which locality is important. Further work with student-teacher models however experimented with training student models that were thinner but deeper. Training of these /FitNets/ was enabled by not only making the student mimic the outputs of the teacher network through distillation but also performing a step of pre-training where an intermediate layer in the student network is tasked with mimicking an early layer in the teacher, thus making the student learn the internal representations of the data from the teacher[[bibentry:romero2014fitnets]]. This process is called /hint-based training/ and manages to produce students with 10 times less parameters but performance that either matches or in some cases even outperform their teachers.
Student-teacher learning is a type of model compression where a smaller and/or faster to compute /student/ network is trained by making it learn the representations learned by a larger /teacher/ network. This idea was first introduced for compressing ensemble models produced by /Ensemble Selection/[[bibentry:caruana2004ensemble]] which consist of hundreds of models of many different kinds, support vector machines, neural networks, memory based models, and decision trees into a single neural network[[bibentry:bucilua2006model]]. This work leverages the neural networks property of being universal approximators[[bibentry:cybenko1989approximation]], meaning that given sufficiently much training data and a big enough hidden layer a neural network can learn to approximate any function with arbitrary precision, by not directly training the student network om the relatively limited labeled training data available but instead on large amounts of pseudo random data that has been given labels by first being passed through the large teacher ensemble. This compression technique yielded student networks up to 1000 times smaller and 1000 times faster to compute than  their teachers with a negligible drop in accuracy on some test problems.

Further work on student-teacher learning experiments with why deep neural networks usually perform better than shallow ones, even when they have the same amount of parameters, by training shallow student models to mimic deep teachers[[bibentry:ba2014deep]]. This work introduces two major modifications that make training of these student models feasible, firstly the student model isn't tasked with just recreating the same label as the teacher but also the same distribution which is achieved by regressing the student to the logits, log probability, values of the teacher as they were before softmax. Getting predictions from the student is then achieved by adding a softmax layer to the end of it after training. Secondly a bottleneck linear layer is added to the network to speed up training. With these modifications they are able to train flat neural networks for both the TMIT and CIFAR-10 datasets with performance closely matching that of single deep networks. Continued analysis of flat networks however shows that depth and convolutions are critical for getting good performance on image classification datasets[[bibentry:urban2016deep]]. Empirically this claim is supported by training state of the art, deep, convolutional models for classification on the CIFAR-10 dataset and then building an ensemble of such models using that as a teacher for shallow students. The student models were then compared to deep convolutional benchmarks that were not trained in a student-teacher fashion. To make sure that the networks were all performing to the best of their abilities and thus making the comparison fair Bayesian hyperparameter optimization[[bibentry:snoek2012practical]] was used. Through this thorough analysis it was found that shallow networks are unable to mimic the performance of deep networks if the number of parameters is held constant between them, these findings are also in agreement with the theoretical results that the representational efficiency of neural networks grows exponentially with layers[[bibentry:liang2016deep]].

Improvements to the student-teacher learning method have been proposed where the student is tasked with minimizing the weighted average of the cross-entropy between its own output and the teacher output when the last layer is softmax with increased temperature, yielding softer labels, and the cross-entropy between the student output and the correct labels when they are available. This framework is called /Distillation/[[bibentry:hinton2015distilling]] and proves to work very well for transferring of information from teacher to student which is demonstrated by training a student model with only 13.2% test error on the MNIST dataset despite only having seen 7s and 8s during its own training. These results mean that distillation manages to transfer knowledge about how a 6 looks from the teacher to the student by only telling it to what degree different 7s and 8s don't look like 6s.

Continued work lead to the creation of /FitNets/[[bibentry:romero2014fitnets]] which goes in the opposite direction to previous attempts at student architectures and instead proposes very deep but thin students. To enable learning in these deep student networks a stage-wise training procedure is used. In the first stage intermediate layers in the teacher and student networks are selected, these are called /hint/ and /guided/ layers respectively. The guided layer in the student is then tasked to mimic the hint layer in the teacher through a convolutional aggressor that compensates for the difference in number of outputs between the networks, this procedure gives a good initialization for the first layers in the student and allows for it to learn the internal representations of the data from the teacher. The second stage of training is then distillation as described above but with the small addition that the weight of the loss against the teacher is slowly annealed during training. This annealing allows for the student to lean heavily on the teacher for support in early stages of training and learn samples which the even the teacher struggles with towards the end of its training. Using this approach the /FitNets/ manage to produce predictions at the same level or in some cases even better than models with 10 times more parameters.

Some of the most resent work in student-teacher learning leverages /attention/ mechanisms for the transfer of knowledge allowing the student not only to learn what the teacher sees but also where in the image the teacher looks to see that[[bibentry:zagouruyko2017paying]][fn:1]. 

** Architectural optimizations
# /SqueezeNet/[[bibentry:iandola2016squeezenet]] presents a different take on how to get smaller models in that it rather optimizes the architecture of the network than any of the constituent parts, this approach gives a network with AlexNet performance but with 50 times fewer parameters than normal AlexNet. This is done by focusing on the usage of \(1 \times 1\) convolutional filters, reducing the amount of channels that go in to the larger filters and by holding out on downsampling so that feature maps are kept large through the network. It was also proven that these results were orthogonal from compression by running the SqueezeNet through the compression framework presented in[[bibentry:han2015deep]] and getting further 10 times compression with out accuracy loss.

# Another orthogonal approach for compression is to optimize the convolutional layers them selves making them require less parameters or less computation to perform their tasks but still keep as much as possible of their recreational power. One of the simplest things that can be done here is to replace single layers of \(N \times N\) convolutional filters with two layers with \(N \times 1\) and \(1 \times N\) filters respectively this reduces the amount of parameters that have to be stored per channel from \(N^2\) to \(2N\) and the amount of multiplications that have to be made scale in the same way. This approach has seen successful use in inception models[[bibentry:szegedy2016rethinking]]. 
# Other variations on the convolutional operator that help compress the networks are dilated convolutions[[bibentry:yu2015multi]] where an exponentially expanding receptive field is achieved without the need for any extra parameters. There have also been some promising results from /depthwise separable convolutions/ where the convolution is factored into a depthwise convolution followed by a pointwise \(1 \times 1\) convolution reducing the computational load with a factor \(8\) to \(9\) for \(3 \times 3\) convolutional kernels[[bibentry:howard2017mobilenets]]. This scheme was introduced in[[bibentry:sifre2014rigid]] and has since seen been successfully used in /MobileNets/[[bibentry:howard2017mobilenets]] and /Inception/ models[[bibentry:ioffe2015batch]]. 

[[bibliographystyle:plain]]
[[bibliography:bibliography.bib]] 

* Footnotes

[fn:1] måste läsa och faktiskt kunna skiten för att sammanfatta va

