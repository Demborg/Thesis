#+TITLE: Related Work
#+AUTHOR: Axel Demborg \texttt{demborg@kth.se}
#+LATEX_HEADER: \newcommand{\bibentry}[1]{\cite{#1}}
#+OPTIONS: toc:nil num:nil

* Related work
Since their introduction Convolutional Neural Networks[[bibentry:lecun1998gradient]] have driven the state of the art for computer vision systems. 
# blal bla grejer om previous work p책 object segmentation, awsome hut sadly they are huge!

Modern neural networks are usually based on 32-bit floating point representations of parameters. It has been shown however that networks are quite resilient to noise[fn:2] and that the networks can be compressed by changing to a less accurate format without any loss in performance. This can be done either by reducing the bit accuracy after training[fn:3] or by doing the entire training in reduced accuracy to. The benefits of using a reduced format like this for representation is not only that the models take less space but also that the individual multiplications become cheaper and hence the networks run faster[fn:4].

It was shown in[[bibentry:denil2013predicting]] that neural networks typically contain a lot of redundant parameters and that up to 95% of parameters could be predicted from the remaining 5% without any drop in accuracy. One of the most direct ways of reducing the number of parameters in the models are the ones where the weight matrices in the networks are dealt with directly, either by grouping weights directly during training with hashing functions [[bibentry:chen2015compressing]] or by clustering the weights after training[[bibentry:gong2014compressing]]. Both these approaches reported compression rates of about 20 times before any significant drop in accuracy was introduced which seems to confirm the results from[[bibentry:denil2013predicting]]. Further work in this area [[bibentry:han2015learning]] explores the effect of pruning away weights close to zero and thus achieves sparse weight matrices which are not only smaller to store but also faster to compute. Combined with some other tricks like /weight sharing/ and /Huffman coding/ this lets[[bibentry:han2015deep]] compress the storage required for AlexNet by a factor 35 and reduce the computation required by a factor 3 without any loss in accuracy on the ImageNet dataset.

A different approach for compressing neural networks is to train a small /student/ model to mimic a larger /teacher/ and thus learn its representations in a more compact form. This idea was introduced in[[bibentry:bucilua2006model]] to compress an ensemble of models into a single smaller one. Further work along these lines trains very flat student architectures from teachers with deep architectures by regressing to the logits outputs of the teacher. This approach yields fully connected shallow networks with performance previously only achievable from deep convolutional networks [[bibentry:ba2014deep]]. Improvements to the training of the students were proposed in[[bibentry:hinton2015distilling]] where regressing to the logits output from the teacher was replaced with a novel approach called /distillation/ where the loss function for the student instead is defined as a weighted sum between the cross entropy between the increased temperature softmax outputs from the student and teacher outputs and the cross entropy with the correct labels. 
The results from[[bibentry:ba2014deep]] were somewhat disproved by [[bibentry:urban2016deep]] where it is shown that depth and convolutions are central when working with image data in which locality is important. Further work with student-teacher models however experimented with training student models that were thinner but deeper. Training of these /FitNets/ was enabled by not only making the student mimic the outputs of the teacher network through distillation but also performing a step of pre-training where an intermediate layer in the student network is tasked with mimicking an early layer in the teacher, thus making the student learn the internal representations of the data from the teacher[[bibentry:romero2014fitnets]]. This process is called /hint-based training/ and manages to produce students with 10 times less parameters but performance that either matches or in some cases even outperform their teachers.

Another orthogonal approach for compression is to optimize the convolutional layers them selves making them require less parameters or less computation to perform their tasks but still keep as much as possible of their recreational power. One of the simplest things that can be done here is to replace single layers of \(N \times N\) convolutional filters with two layers with \(N \times 1\) and \(1 \times N\) filters respectively this reduces the amount of parameters that have to be stored per channel from \(N^2\) to \(2N\) and the amount of multiplications that have to be made scale in the same way[fn:1].
Other variations on the convolutional operator that help compress the networks are dilated convolutions 

[[bibliography:bibliography.bib]] 
[[bibliographystyle:plain]]

* Footnotes

[fn:4] mer k채lla!

[fn:3] k채lla

[fn:2] k채lla!

[fn:1] THIS desperately needs a good reference and stuff!
