\documentclass{kththesis}


\usepackage{csquotes} % Recommended by biblatex
\usepackage{biblatex}
\addbibresource{bibliography.bib} % The file containing our references, in BibTeX format

% My stuff
\usepackage{amsmath}
\usepackage{amssymb}
\newcommand{\bibentry}[1]{\parencite{#1}}


\title{Real-time segmentation on smartphone}
\alttitle{Realtids segmentering p√• smartphone}
\author{Axel Demborg}
\email{demborg@kth.se}
\supervisor{Hossein Azizpour}
\examiner{Danica Kragic}
\programme{Master in Computer Science}
\school{School of Electrical Engineering and Computer Science}
\date{\today}


\begin{document}

% Frontmatter includes the titlepage, abstracts and table-of-contents
\frontmatter

\titlepage

\begin{abstract}

\end{abstract}


\begin{otherlanguage}{swedish}
  \begin{abstract}
  \end{abstract}
\end{otherlanguage}


\tableofcontents


% Mainmatter is where the actual contents of the thesis goes
\mainmatter


\chapter{Introduction}



\section{Research Question}

\chapter{Background}
\section{Related work}
Convolutional Neural Networks (CNN) where first introduced in 1998 \bibentry{lecun1998gradient} and since then larger and larger CNNs have slowly become the state of the art method for most areas of computer vision. Notably \emph{AlexNet} \bibentry{krizhevsky2012imagenet} in 2012 proved that that deep CNNs could be used for high resolution image classification by beating the previous state of the art \bibentry{sanchez2011high} on the \emph{ImageNet} classification challenge \bibentry{deng2009imagenet}. To do this \emph{AlexNet} used 60 million parameters and 650,000 neurons and training of the network was only made feasible by the use of multiple graphical processing units (GPUs) \bibentry{krizhevsky2012imagenet}. 

In the areas of object detection and semantic segmentation it was \emph{Regions with CNN features} (\emph{R-CNN}) \bibentry{girshick2014rich} that in 2014 first showed that CNNs could be successfully be applied to these fields by significantly improving over the previous state of the art in object detection \bibentry{ren2013histograms} and after minor modifications matching the performance of the state of the art in semantic segmentation \bibentry{carreira2012semantic} with a system not specifically built for the task. For object detection \emph{R-CNN} works as a hybrid system with \emph{selective search} \bibentry{uijlings2013selective} producing proposals for object regions and a CNN, pre-trained on \emph{ImageNet} \bibentry{deng2009imagenet} and fine-tuned for region classification, generating fixed length features for each region, finally classifying each region by running class specific \emph{support vector machines} \bibentry{boser1992training} on these features. Some issues with \emph{R-CNN} are that it requires a multistage training, training the CNN to give good features and training the SVMs for classification, and that it is slow, for training but most notably at inference where one image is processed in 47s. These problems are addressed with further work resulting in \emph{Fast R-CNN} \bibentry{girshick2015fast} where the CNN isn't run once per proposed region but instead once for the entire network generating a convolutional feature map that is then pooled with a region of interest (RoI) pooling layer to produce a feature vector for each region. These feature vectors are then feed into a fully connected neural network with two sibling output layers that perform both classification and bounding box refinement in parallel. With these improvements \emph{Fast R-CNN} achieves faster inference and higher accuracy than its predecessors and does so with a arguably much more elegant design. Even though \emph{Fast R-CNN} improved speed significantly it was nowhere near real-time performance, further performance improvements were introduced with \emph{Faster R-CNN} \bibentry{ren2015faster} where \emph{selective search} for region proposals is replaced with Region Proposal Networks, fully convolutional neural networks that take as input the convolutional feature maps as described from \emph{Fast R-CNN} and outputs region proposals. Since this approach for region proposals shares most of its computation with the classification network the region proposals are practically free and frame rates of 5fps are achievable. The region proposal networks not only speed up computation but also prove to give better accuracy region proposals and thus raise over all accuracy in the system as well \bibentry{ren2015faster}. Even further improvements to this framework was achieved with the introduction of \emph{Mask R-CNN} \bibentry{he2017mask} which expands upon \emph{Faster R-CNN} by adding a third branch for a segmentation mask besides the branches for bounding box refinement and classification making the system able to predict not only the general bounding box of items in the image but also which exact pixels belong to the object. Since segmentation is a pixel-by-pixel prediction problem \emph{Mask R-CNN} replaces the spatially quantizing RoIPool operation from \emph{Fast R-CNN} with a quantization-free layer called RoIAllign.

Some parallel work on semantic segmentation of images resulted in \emph{SegNet} \bibentry{badrinarayanan2015segnet}, a fully convolutional encoder-decoder network. Here the encoder network encodes the input image down into a lower dimensional feature space while keeping storing the indices of the max pooling operations. The low dimensional representations are then run through a decoder network which is architecturally a mirror image of the encoder network but where the max pooling operations have been replaced with upsampling layers that use the stored indices from the corresponding pooling layers to maintain the granularity of the images. The final layer in the network is a softmax and hence the outputs are the probabilities of each pixel belong to each class.
Continued work on segmentation utilizes blocks of so called \emph{DenseNets} \bibentry{huang2017densely}, CNNs where every layer is connected to every layer after it enabling the training of really deep network architectures by alleviating the vanishing gradient problem and promoting feature reuse between the layers. By using these \emph{DensNets} in a very deep encoder-decoder structure where skip connections restore image granularity during upsampling the state of the art in image segmentation has been pushed even further \bibentry{jegou2017one} while still reducing the amount of parameters required for the models by a factor 10 as compared to the previous state of art.

Despite their impressive performance on a wide range of problems neural networks are still prohibited from running locally on mobile devices with slow processors, limited power envelopes or limited memory due to their large size and big computational load. For example modern neural networks can't fit on the on-chip SRAM cache and instead they have to reside in the much more power hungry off-chip DRAM memory making applications up to 100 times more power consuming \bibentry{han2015learning}. Regarding inference speed the most modern networks for object segmentation \bibentry{he2017mask} run at 5fps but that is on high performance GPUs meaning that mobile performance is far from real-time. Due to these limitations applications of neural networks for mobile use cases are either to forced give up on state of the art performance or to be run on off-site servers which requires steady network connections and incurs delays, both of which may be intolerable for real-time mobile applications, self driving cars and robotics \bibentry{jin2014flattened}. However work on understanding the structure of the learned weights in neural networks \bibentry{denil2013predicting} has showed that there is significant redundancy in the parameterization of several deep learning models and that up to 95\% of weights in networks can be predicted from the remaining 5\% without any drop in accuracy. This indicates that models could be made much smaller while still maintaining performance and several such approaches for squeezing high performance networks into small memory footprints and computational loads have been proposed. The most prominent approaches will be presented below. 

\subsection{Quantization of weights}
Modern neural networks are usually based on 32-bit floating point representations of parameters. It has been shown however that networks are quite resilient to noise and even that some noise can improve training \bibentry{murray1994enhanced}. Since reduced precision variables can be modeled as noise this means that networks can be compressed by changing to a less accurate format without any loss in performance. This can be done either by reducing the bit accuracy after training \bibentry{vanhoucke2011improving}  or by doing the entire training in reduced accuracy \bibentry{hubara2016quantized} \bibentry{gupta2015deep}. The benefits of using a reduced format like this for representation is not only that the models take less space but also that the individual multiplications become cheaper and hence the networks run faster.

\subsection{Weight sharing}
One of the most direct approaches for removing the redundancy in parametrization from neural networks is by forcing the networks to share weights between different connections. This is precisely what \emph{HashedNets} \bibentry{chen2015compressing} does by fixing the amount of weights \(K^l\) that are to be used in each layer making the weights \(\vec{w^l} \in \mathbb{R}^{K^l}\) and using hashing functions to map each element in the virtual weight matrices \(V_{ij}^l\) to one of these weights \(V_{ij} = w_{h(i,j)}\) with \(h()\) being a hashing function. With the weight matrices defined in this fashion \emph{HashedNets} can be trained like normal networks with the gradients with respect to the weights calculated from the gradients with respect to the virtual matrices as 
\[ \frac{\partial\mathcal{L}}{\partial w_k^l} = \sum_{ij} \frac{\partial\mathcal{L}}{\partial V_{ij}^l}\frac{\partial V_{ij}^l}{\partial w_k^l} \]
This method gave a compression of about 20 times before any notable loss in accuracy was introduced during tests on variations of the MNIST dataset which seems to agree very well with the results from \bibentry{denil2013predicting}.

Other notable work focuses on the use of k-means clustering to cluster the weights in networks after training \bibentry{gong2014compressing}, this proves to work very well and manages to compress the models with a factor 16 with no more than a 0.5\% drop in classification accuracy on the ImageNet dataset. Further work in this area explores the effects of pruning away low-weight connections and iterative retraining of the pruned networks \bibentry{han2015learning}. This lets the authors compress models with a factor 9 - 13 without any loss in performance while getting sparser weight matrices that could potentially speed up calculations. These two lines of research, clustering and pruning, where merged into a single framework called \emph{deep compression} \bibentry{han2015deep} where a three stage approach is taken to model compression. First low-weight connections are pruned away and the network is retrained to compensate for this, in the second stage k-means clustering is performed on the weights and again the network is retrained to make the clusters take the most useful values, finally Huffman coding \bibentry{van1976construction} is used to reduce the storage required for the weights. This process allows \emph{deep compression} to compress networks with a factor 35 without any loss in accuracy. Despite these very impressive results however \emph{deep compression} comes with a major drawback, it can't be run efficiently in its compressed form and the full weight matrices have to be rebuilt at inference time to use the models on commodity hardware. To alleviate these problems hardware has been designed that could perform prediction directly from the compressed models. This so called \emph{efficient inference engine} \bibentry{han2016eie} would enable inference 13 times faster than GPU while being 3400 times more energy efficient.

\subsection{Student-teacher learning}
Student-teacher learning is a type of model compression where a smaller and/or faster to compute \emph{student} network is trained by making it learn the representations learned by a larger \emph{teacher} network. This idea was first introduced for compressing ensemble models produced by \emph{Ensemble Selection} \bibentry{caruana2004ensemble} which consist of hundreds of models of many different kinds, support vector machines, neural networks, memory based models, and decision trees into a single neural network \bibentry{bucilua2006model}. This work leverages the neural networks property of being universal approximators \bibentry{cybenko1989approximation}, meaning that given sufficiently much training data and a big enough hidden layer a neural network can learn to approximate any function with arbitrary precision, by not directly training the student network on the relatively limited labeled training data available but instead on large amounts of pseudo random data that has been given labels by first being passed through the large teacher ensemble. This compression technique yielded student networks up to 1000 times smaller and 1000 times faster to compute than  their teachers with a negligible drop in accuracy on some test problems.

Further work on student-teacher learning experiments with why deep neural networks usually perform better than shallow ones, even when they have the same amount of parameters. This was done by training shallow student models to mimic deep teachers \bibentry{ba2014deep}. The work introduces two major modifications that make training of these student models feasible, firstly the student model isn't tasked with just recreating the same label as the teacher but also the same distribution which is achieved by regressing the student to the logits, log probability, values of the teacher as they were before softmax. Getting predictions from the student is then achieved by adding a softmax layer to the end of it after training. Secondly a bottleneck linear layer is added to the network to speed up training. With these modifications they are able to train flat neural networks for both the TMIT and CIFAR-10 datasets with performance closely matching that of single deep networks. Continued analysis of flat networks however shows that depth and convolutions are critical for getting good performance on image classification datasets \bibentry{urban2016deep}. Empirically this claim is supported by training state of the art, deep, convolutional models for classification on the CIFAR-10 dataset and then building an ensemble of such models using that as a teacher for shallow students. The student models were then compared to deep convolutional benchmarks that were not trained in a student-teacher fashion. To make sure that the networks were all performing to the best of their abilities and thus making the comparison fair Bayesian hyperparameter optimization \bibentry{snoek2012practical} was used. Through this thorough analysis it was shown that shallow networks are unable to mimic the performance of deep networks if the number of parameters is held constant between them, these findings are also in agreement with the theoretical results that the representational efficiency of neural networks grows exponentially with the number of layers \bibentry{liang2016deep}.

Improvements to the student-teacher learning method have been proposed where the student is tasked with minimizing the weighted average of the cross-entropy between its own output and the teacher output when the last layer is softmax with increased temperature, yielding softer labels, and the cross-entropy between the student output and the correct labels when they are available. This framework is called \emph{Distillation} \bibentry{hinton2015distilling} and proves to work very well for transferring of information from teacher to student. The framework is demonstrated by training a student model with only 13.2\% test error on the MNIST dataset despite only having seen 7s and 8s during its own training. These results mean that distillation manages to transfer knowledge about how a 6 looks from the teacher to the student by only telling it to what degree different 7s and 8s don't look like 6s.

Continued work lead to the creation of \emph{FitNets} \bibentry{romero2014fitnets} which goes in the opposite direction to previous attempts at student architectures and instead proposes very deep but thin students. To enable learning in these deep student networks a stage-wise training procedure is used. In the first stage intermediate layers in the teacher and student networks are selected, these are called \emph{hint} and \emph{guided} layers respectively. The guided layer in the student is then tasked to mimic the hint layer in the teacher through a convolutional regressor that compensates for the difference in number of outputs between the networks, this procedure gives a good initialization for the first layers in the student and allows for it to learn the internal representations of the data from the teacher. The second stage of training is then distillation as described above but with the small addition that the weight of the loss against the teacher is slowly annealed during training. This annealing allows for the student to lean heavily on the teacher for support in early stages of training and learn samples which the even the teacher struggles with towards the end of its training. Using this approach the \emph{FitNets} manage to produce predictions at the same level or in some cases even better than models with 10 times more parameters.

Some more recent work \bibentry{zagouruyko2017paying} builds upon the ideas from \emph{FitNets} with not only letting the students mimic the output of teachers but also some intermediary representations. Unlike the way it is done \emph{FitNets} however the student is not tasked with reconstructing the exact activations of the teacher in the intermediate layers but instead the attention maps, regions in the image that the teacher uses to make its predictions, and thus teaches the student where to look. A few different methods for calculating these attention maps are proposed in the paper but notable is that they are all non parametric meaning that no extra layers of convolution have to be learned to make the student attention maps comparable to the ones from the teacher. This attention transferring approach is proves to give good results on a number of difficult datasets including \emph{ImageNet} and is also shown to work well together with distillation.

\subsection{Architectural optimizations}
Another orthogonal approach for compression is to optimize the convolutional layers them selves making them require less parameters or less computation to perform their tasks but still keep as much as possible of their representational power. One of the simplest things that can be done here is to replace single layers of \(N \times N\) convolutional filters with two layers with \(N \times 1\) and \(1 \times N\) filters respectively, this reduces the amount of parameters that have to be stored per channel from \(N^2\) to \(2N\) and the amount of multiplications that have to be made scale in the same way. These asymmetrical convolutions have seen successful use in inception models \bibentry{szegedy2016rethinking}. 
Other variations on the convolutional operator that help compress the networks are dilated convolutions \bibentry{yu2015multi} where an exponentially expanding receptive field is achieved without the need for any extra parameters. There have also been some promising results from \emph{depthwise separable convolutions} where the convolution is factored into a depthwise convolution followed by a pointwise \(1 \times 1\) convolution reducing the computational load with a factor \(8\) to \(9\) for \(3 \times 3\) convolutional kernels \bibentry{howard2017mobilenets}. This scheme was introduced in \bibentry{sifre2014rigid} and has since seen been successfully used in \emph{Inception} models \bibentry{ioffe2015batch}. 

\emph{SqueezeNet} \bibentry{iandola2016squeezenet} presents a different take on how to get smaller models in that it rather optimizes the architecture of the network than any of the constituent parts, this approach gives a network with \emph{AlexNet} performance but with 50 times fewer parameters than normal \emph{AlexNet}. This is done by focusing on the usage of \(1 \times 1\) convolutional filters, reducing the amount of channels that go in to the larger filters and by holding out on downsampling so that feature maps are kept large through the network. It was also proven that these results were orthogonal from compression by running the SqueezeNet through the \emph{deep compression} framework \bibentry{han2015deep} and getting further 10 times compression with out accuracy loss.

\emph{MobileNets} \bibentry{howard2017mobilenets} combine these two approaches, utilizing both \emph{depthwise separable convolutions} and a heavily optimized architecture to build networks specially suited for mobile vision applications. In doing so \emph{MobileNets} also introduce two hyper-parameters, \emph{width-multiplier} and \emph{resolution-multiplier} that help design models with a optimal trade off between latency and precision given the limitations of the available hardware.

Another network specially designed for real-time segmentation on mobile devices is \emph{ENet} \bibentry{paszke2016enet}. Here dilated convolutions are used together with asymmetrical convolutions to give a large receptive field without introducing that many parameters. The network is built as an encoder-decoder network but with a much smaller decoder, the argument behind this being that decoder should simply upsample the output while fine-tuning the details which should be a simpler task than the information processing and extraction that the encoder is performing. Attention has also been payed to quickly downsampling the feature maps which saves on computation but then not downsampling so aggressively after that keeping much of the spatial information in the images. Together these improvements give a network that performs on par with \emph{SegNet} but that requires 79 times fewer parameters and is 18 times faster at inference.



\chapter{Methods}

\chapter{Results}

\chapter{Discussion}

\printbibliography[heading=bibintoc] % Print the bibliography (and make it appear in the table of contents)

\appendix

\chapter{Unnecessary Appended Material}

\end{document}
